{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be21780c",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "**Notebook 2: Data Preprocessing**\n",
    "\n",
    "This notebook cleans the data, creates weighted interactions based on ratings, and performs temporal train/validation/test splits to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4089ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries and data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "reviews = pd.read_parquet('data/reviews.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af06a30",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636f1133",
   "metadata": {},
   "source": [
    "For collaborative filtering to work properly, we'll require at most one interaction per user per item. Also, in my production dataset, we dropped beers without ABV information to preserve data quality, so we'll do the same here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9ec1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data:\n",
    "# 1. Drop rows with missing values (removes ~82k rows with missing user/brewery/ABV)\n",
    "# 2. Sort by user, date, beer_id for chronological processing\n",
    "# 3. Remove duplicate reviews, keeping the latest review per user-beer pair\n",
    "reviews.dropna(inplace=True)\n",
    "reviews.sort_values(by=['user', 'date', 'beer_id'], inplace=True)\n",
    "reviews.drop_duplicates(subset=['user', 'beer_id'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e4f2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pre-filter count \n",
    "pre_filter_count = reviews.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d790e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures that users have minimum interaction history\n",
    "item_counts = reviews['beer_id'].value_counts()\n",
    "filtered = reviews.loc[lambda x: x[\"beer_id\"].map(item_counts) >= 5]\n",
    "user_counts = filtered['user'].value_counts()\n",
    "filtered = filtered.loc[lambda x: x[\"user\"].map(user_counts) >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f503d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1418686\n",
      "Retained 94.33% of reviews after filtering.\n"
     ]
    }
   ],
   "source": [
    "# After cleaning: 1418686 reviews (94.33% retention)\n",
    "print(filtered.shape[0])\n",
    "print(f\"Retained {filtered.shape[0] / pre_filter_count:.2%} of reviews after filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f11e78be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005110646019842053"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final sparsity: 0.51% - improved density for training\n",
    "len(filtered) / (len(filtered['user'].unique()) * len(filtered['beer_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c616592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1\n"
     ]
    }
   ],
   "source": [
    "# We improved the data sparsity by 7.1x the original!\n",
    "print(round(0.005110646019842053 / 0.0007194294107235213, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba4e351",
   "metadata": {},
   "source": [
    "### Creating Weighted Implicit Feedback\n",
    "\n",
    "**Why transform explicit ratings into weights?**\n",
    "\n",
    "LightFM works with feedback where interactions have varying confidence levels. We transform 5-star ratings into weights:\n",
    "\n",
    "| Rating Range | Weight | Interpretation |\n",
    "|--------------|--------|----------------|\n",
    "| 0-2 stars | 0.0 | Negative signal (user dislikes this beer) |\n",
    "| 2-3 stars | 0.01 | Weak positive (tried it, not impressed) |\n",
    "| 3-4 stars | 0.09 | Moderate positive (liked it) |\n",
    "| 4-5 stars | 0.9 | Strong positive (loved it) |\n",
    "\n",
    "This allows the model to distinguish between good and bad experiences.\n",
    "\n",
    "Note that we choose to weight negative interactions by 0. This is because the loss functions implemented by LightFM will treat interacted-with items as \"positive interactions\" and the uninteracted with items as \"negative interactions\". In this case, we don't want beers users disliked to be counted as positive interactions. For beers users tried but didn't like very much, we interpret it as a weak signal (the user cared enough to try the beer even if they didn't love it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b797abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered['weight_all'] = pd.cut(\n",
    "    filtered['rating'],\n",
    "    bins=[-float('inf'), 2, 3, 4, float('inf')],\n",
    "    labels=[0, 0.01, 0.09, .9],\n",
    "    right=False\n",
    ").astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619c506",
   "metadata": {},
   "source": [
    "### Temporal Train/Val/Test Split\n",
    "\n",
    "**Why temporal splitting?**\n",
    "\n",
    "Unlike random splitting, temporal splits simulate real-world deployment. The goal of recommender systems is to predict what users will want to try next, based on their interaction history.\n",
    "\n",
    "**Split strategy:**\n",
    "- Power users (â‰¥300 reviews): reserve last 200 reviews for val/test (100 each)\n",
    "- Regular users (<300 reviews): all data goes to training\n",
    "\n",
    "This approach saves as much data as possible for training (necessary due to sparse data), while reserving validation data for users with enough history to meaningfully measure their taste preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21965aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal splitting into train/val/test sets\n",
    "filtered_sorted = filtered.sort_values(['user', 'date'])\n",
    "\n",
    "# Initialize empty lists for splits\n",
    "train, val, test = [], [], []\n",
    "\n",
    "# Group by user and split based on interaction count\n",
    "for user, group in filtered_sorted.groupby('user'):\n",
    "    n = len(group)\n",
    "    if n >= 300:\n",
    "        train.append(group.iloc[:-200])\n",
    "        val.append(group.iloc[-200:-100])\n",
    "        test.append(group.iloc[-100:])\n",
    "    else:\n",
    "        train.append(group)\n",
    "\n",
    "# Concatenate all user splits into final datasets\n",
    "train = pd.concat(train)\n",
    "val = pd.concat(val)\n",
    "test = pd.concat(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f493117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length: 1180486\n",
      "val length: 119100\n",
      "test length: 119100\n"
     ]
    }
   ],
   "source": [
    "# view dataset sizes\n",
    "print(f\"train length: {train.shape[0]}\")\n",
    "print(f\"val length: {val.shape[0]}\")\n",
    "print(f\"test length: {test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b123a99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items in train: 19110\n",
      "items in validation: 14435\n",
      "items in test: 14263\n",
      "users in train: 14492\n",
      "users in validation: 1191\n",
      "users in test: 1191\n"
     ]
    }
   ],
   "source": [
    "# item coverage\n",
    "train_items = set(np.unique(train['beer_id']))\n",
    "val_items = set(np.unique(val['beer_id']))\n",
    "test_items = set(np.unique(test['beer_id']))\n",
    "\n",
    "print(f\"items in train: {len(train_items)}\")\n",
    "print(f\"items in validation: {len(val_items)}\")\n",
    "print(f\"items in test: {len(test_items)}\")\n",
    "\n",
    "# user coverage\n",
    "train_users = set(np.unique(train['user']))\n",
    "val_users = set(np.unique(val['user']))\n",
    "test_users = set(np.unique(test['user']))\n",
    "\n",
    "print(f\"users in train: {len(train_users)}\")\n",
    "print(f\"users in validation: {len(val_users)}\")\n",
    "print(f\"users in test: {len(test_users)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0cc973",
   "metadata": {},
   "source": [
    "### Cold-Start Items\n",
    "\n",
    "If any beers appear in val/test but not in the training set, our model won't be able to generate predictions for those items because it only learns embeddings for items it trains on. Therefore, we'll exclude these items when evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "095924c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# save cold-start items for evaluation exclusion\n",
    "cold_start_items = pd.DataFrame((val_items.union(test_items)) - train_items)\n",
    "# There are no cold start items in train or test!\n",
    "print(val['beer_id'].isin(cold_start_items).sum())\n",
    "print(test['beer_id'].isin(cold_start_items).sum())\n",
    "\n",
    "# there is nothing to remove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d57868a",
   "metadata": {},
   "source": [
    "### Final Dataset Split\n",
    "\n",
    "Evaluation framework:\n",
    "\n",
    "| Split | Interactions | Users | Items |\n",
    "|-------|-------------|-------|-------|\n",
    "| **Train** | 1,180,486 | 14,492 | 19,110 |\n",
    "| **Validation** | 119,100 | 1,191 | 14,435 |\n",
    "| **Test** | 119,100 | 1,191 | 14,263 |\n",
    "\n",
    "**Key properties:**\n",
    "- Balanced val/test sets (same size, same users)\n",
    "- Temporal ordering preserved (train < val < test in time)\n",
    "- Ready for model training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d700ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all processed datasets for model training\n",
    "filtered.to_parquet('data/filtered.parquet', engine='pyarrow', index=False)\n",
    "train.to_parquet('data/train.parquet', engine='pyarrow', index=False)\n",
    "val.to_parquet('data/val.parquet', engine='pyarrow', index=False)\n",
    "test.to_parquet('data/test.parquet', engine='pyarrow', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
