{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae23a78",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "**Notebook 3: Model Training**\n",
    "\n",
    "This notebook trains two LightFM models using Optuna for hyperparameter optimization:\n",
    "1. **Baseline model**: collaborative filtering only (user-item interactions)\n",
    "2. **Feature-enhanced model**: adds item features (beer style + brewery)\n",
    "\n",
    "LightFM uses matrix factorization with WARP/BPR loss to learn user and item embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33427349",
   "metadata": {},
   "source": [
    "**NOTE: This training sequence uses the `lightfm-next` package to allow forwards-compatibility with python 3.10+ which the base LightFM package lacks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6285fe",
   "metadata": {},
   "source": [
    "### Discusssion on Feature Selection\n",
    "\n",
    "The first model will not use item features, making it a pure matrix factorization (MF) model which corresponds to the SVD model. This model will only train on interaction (rating) data. The second model will learn brewery and style metadata embeddings, which should be informative/contain useful signals as some users will prefer beers from certain breweries or styles. The item features should also improve the item-item similarity results. Based on the results of our EDA, we want to avoid giving the model weak/noisy signals as the data is already sparse. Therefore, we've opted to not engineer or use additional features. For example, we'll leave out ABV as it is most likely a noisy variable to include. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0d886",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33aa014f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kitsuragi/Desktop/Code/Brewtiful/.venv/lib/python3.10/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import LightFM and evaluation libraries\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightfm\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "from lightfm import LightFM\n",
    "from lightfm import data\n",
    "from lightfm.evaluation import precision_at_k, recall_at_k, auc_score, reciprocal_rank\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from unidecode import unidecode # to deal with accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b0260ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed train/val/test splits + datasets\n",
    "train = pd.read_parquet('data/train.parquet', engine='pyarrow')\n",
    "val = pd.read_parquet('data/val.parquet', engine='pyarrow')\n",
    "test = pd.read_parquet('data/test.parquet', engine='pyarrow')\n",
    "filtered = pd.read_parquet('data/filtered.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "296993e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LightFM Dataset object for ID mapping\n",
    "light_data = lightfm.data.Dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39cc184",
   "metadata": {},
   "source": [
    "### LightFM Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8688b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to fit the dataset with all users and items in the training set\n",
    "light_data = lightfm.data.Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81c3c1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we'll create a list of all unique item features (styles + breweries)\n",
    "metadata = list(set(filtered['style'].unique()).union(set(filtered['brewery'].unique())))\n",
    "# Now, fit the dataset with users, items, and item features from the training set\n",
    "light_data.fit(users=filtered['user'].unique(),\n",
    "               items=filtered['beer_id'].unique(),\n",
    "               item_features=metadata)\n",
    "# Store item mappings for later use\n",
    "user_mappings = light_data._user_id_mapping\n",
    "item_mappings = light_data._item_id_mapping\n",
    "inv_user_mappings = {v:k for k, v in user_mappings.items()}\n",
    "inv_item_mappings = {v:k for k, v in item_mappings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba15749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build interaction matrices with sample weights from rating bins\n",
    "# Train uses weights (0, 0.01, 0.09, 0.9), validation is binary\n",
    "train_interactions = light_data.build_interactions(train[['user', 'beer_id', 'weight_all']].values)\n",
    "val_interactions = light_data.build_interactions(val[['user', 'beer_id']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75119313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kitsuragi/Desktop/Code/Brewtiful/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Construct item feature matrix with helper function\n",
    "from helpers.training import construct_item_features\n",
    "features = construct_item_features(light_data, filtered, b=.2, s=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a28d0ed",
   "metadata": {},
   "source": [
    "### Optimization with Optuna\n",
    "\n",
    "We'll use the `optuna` library to optimize our model with intelligent hyperparameter searching and trial pruning.\n",
    "\n",
    "**Objective**: Maximize Precision@50 on validation set\n",
    "\n",
    "We'll optimize over precision@50 because Brewtiful displays many recommendations at once, coming from diverse style categories. We want to not only show one or two highly relevant items, but multiple relevant items across different categories. 50 should be a sufficiently high k to ensure that we're optimizing the model to select many relevant items.\n",
    "\n",
    "We'll train models both with and without item features to see if the item features help.\n",
    "\n",
    "**Hyperparameters to Optimize**:\n",
    "- no_components: The dimensionality of the feature latent embeddings\n",
    "- learning_schedule: One of (‘adagrad’, ‘adadelta’)\n",
    "- loss: Loss function. We'll test both WARP and BPR (both popular loss functions, but WARP usually performs better when precision is the goal).\n",
    "- learning_rate: Initial learning rate for the adagrad learning schedule.\n",
    "- item_alpha: L2 penalty on item features.\n",
    "- user_alpha: L2 penalty on user features.\n",
    "- max_sampled: Maximum number of negative samples used during WARP fitting.\n",
    "- epochs: Number of epochs to run model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e735da",
   "metadata": {},
   "source": [
    "#### No Item Features (Pure MF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c3688bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-08 17:54:49,488] A new study created in memory with name: no-name-e5cedf27-a494-43f1-9ac5-770b05a11f7f\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from helpers.training import create_objective\n",
    "\n",
    "\n",
    "# Initialize the study\n",
    "init_study = optuna.create_study(direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ae8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for BASELINE model (no item features)\n",
    "# Parameters tuned: embedding size, learning rate, regularization, loss function, epochs\n",
    "# Starting from sensible defaults, then exploring 150 trials\n",
    "# Uses WARP loss (Weighted Approximate-Rank Pairwise) for ranking optimization\n",
    "\n",
    "# Add in our original hyperparmeter values as a starting point for Optuna\n",
    "init_study.enqueue_trial(params={\"no_components\":10, \n",
    "                            \t\t\t\t\t\"learning_schedule\":'adagrad', \n",
    "                            \t\t\t\t\t\"loss\":'warp',\n",
    "                            \t\t\t\t\t\"learning_rate\":0.05,\n",
    "                            \t\t\t\t\t\"item_alpha\":1e-10, \n",
    "                            \t\t\t\t\t\"user_alpha\":1e-10, \n",
    "                            \t\t\t\t\t\"max_sampled\":10,\n",
    "                            \t\t\t\t\t\"epochs\":20})\n",
    "\n",
    "\n",
    "# Run the optimisation        \n",
    "init_study.optimize(create_objective(train_interactions=train_interactions[0], \n",
    "                                val_interactions=val_interactions[0], sample_weight=train_interactions[1], \n",
    "                                use_item_features=False, p_k=50, loss=['warp', 'bpr'],  enable_pruning=True, pruning_interval=5), \n",
    "                                n_trials=150)\n",
    "\n",
    "# Save the best parameters\n",
    "best_params = init_study.best_params\n",
    "for k, v in best_params.items():\n",
    "    print(k,\":\",v)\n",
    "\n",
    "# lengthy cell output cleared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f37070d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best parameters for later use\n",
    "import pickle\n",
    "with open('artifacts/best_params_no_item_features.pkl', 'wb') as f:\n",
    "    pickle.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83226591",
   "metadata": {},
   "source": [
    "#### Item Features\n",
    "\n",
    "This model will not only learn identity embeddings for every user and item, but will also learn embeddings for breweries and styles. However, LightFM gives all features equal weighting by default. It's critical to allow for optimization over the weighting of item features, as the default weighting is likely much too high, hiding the item's unique signal behind their metadata features. We want the metadata features to complement the identity features, not overpower them.\n",
    "\n",
    "**Additional Hyperparameters**:\n",
    "- b: relative weight of brewery embeddings to identity features\n",
    "- s: relative weight of style embeddings to identity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81ba206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-08 20:52:53,005] A new study created in memory with name: no-name-9440778e-1df3-4f90-b778-bc9e7f7087f3\n"
     ]
    }
   ],
   "source": [
    "# Initialize new Optuna study for ITEM FEATURES model\n",
    "# Define the study\n",
    "item_feature_study = optuna.create_study(direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for feature-enhanced model\n",
    "# Additional parameters: b (brewery weight), s (style weight)\n",
    "# These control how much style/brewery metadata influences embeddings\n",
    "\n",
    "# Add in our original hyperparmeter values as a starting point for Optuna\n",
    "item_feature_study.enqueue_trial(params={\"no_components\":10, \n",
    "                            \t\t\t\t\t\"learning_schedule\":'adagrad', \n",
    "                            \t\t\t\t\t\"loss\":'warp',\n",
    "                            \t\t\t\t\t\"learning_rate\":0.05,\n",
    "                            \t\t\t\t\t\"item_alpha\":1e-10, \n",
    "                            \t\t\t\t\t\"user_alpha\":1e-10, \n",
    "                            \t\t\t\t\t\"max_sampled\":10,\n",
    "                            \t\t\t\t\t\"epochs\":20,\n",
    "                                                \"b\": 0.2,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\"s\": 0.1})\n",
    "\n",
    "\n",
    "\n",
    "# Run the optimisation        \n",
    "item_feature_study.optimize(create_objective(train_interactions=train_interactions[0], \n",
    "                                val_interactions=val_interactions[0], sample_weight=train_interactions[1], \n",
    "                                p_k=10, use_item_features = True, light_data=light_data, dataset=filtered,\n",
    "                                loss=['warp', 'bpr'], enable_pruning=True, pruning_interval=5), \n",
    "                                n_trials=150)\n",
    "\n",
    "# Save the best parameters\n",
    "item_feature_best_params = item_feature_study.best_params\n",
    "for k, v in item_feature_best_params.items():\n",
    "    print(k,\":\",v)\n",
    "    \n",
    "# lengthy cell output cleared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e299bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature-enhanced model parameters\n",
    "with open('artifacts/best_params_with_item_features.pkl', 'wb') as f:\n",
    "    pickle.dump(item_feature_best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288b6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_components : 125\n",
      "learning_schedule : adadelta\n",
      "loss : warp\n",
      "item_alpha : 9.311257093797117e-07\n",
      "user_alpha : 5.5829799172677285e-08\n",
      "rho : 0.9049541118958679\n",
      "epsilon : 8.421159595417498e-08\n",
      "max_sampled : 14\n",
      "epochs : 48\n"
     ]
    }
   ],
   "source": [
    "for k, v in best_params.items():\n",
    "    print(k,\":\",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcc47b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_components : 75\n",
      "learning_schedule : adadelta\n",
      "loss : warp\n",
      "item_alpha : 9.854944294071034e-10\n",
      "user_alpha : 9.565341504354942e-07\n",
      "rho : 0.9444957056553378\n",
      "epsilon : 2.878992523820034e-07\n",
      "max_sampled : 11\n",
      "epochs : 46\n",
      "b : 0.009064583858183008\n",
      "s : 0.0005742885432749662\n"
     ]
    }
   ],
   "source": [
    "for k, v in item_feature_best_params.items():\n",
    "    print(k,\":\",v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
